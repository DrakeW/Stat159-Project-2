# Abstract
We perform a predictive modeling process applied on the data set Credit in order to decipher how factors such as Income, Age, Education, and more influence the amount of credit card debt to a person. Our analysis is based off of Chapter 6 from *An Introduction to Statistical Learning*, "Linear Model Selection and Regularization". 

# Introduction

In this project, we applied model selection methods on different regression models introduced in _Chapter 6: Linear Model Selection and Regularization (from **"An Introduction to Statistical Learning"** by James et al)_. The 5 regression models we practiced are _Ordinary Least Squares, Ridge Regression, Lasso Regression, Principle Components Regression and Partial Least Square Regression_. In order to find the best parameter (or coefficients) for those regression models, we splitted our datasets into train and test groups, performed 10-fold cross validation on each model respectively, and in the end chose the best model with the least cross-validation error.

# Data

The Credit data set we will be using comes from *An Introduction to Statistical Learning*, contains the following variables: Income, Limit, Rating, Cards, Age, Education, Gender, Student, Married, Ethnicity and Balance. The Credit data set records Balance, the average credit card debt for an individual, according to quantitative predictors such as Income, Limit, etc. and qualitative predictors such as Student, Married, etc. Cards represents the number of credit cards a person has and Income is represented in the thousands of dollars. We investigate 400 individuals in this analysis. 

# Methods

### Ordinary Least Square methods

In statistics, OLS is a method for estimating the coefficients in a linear regression model, with the goal of minimizing the sum of the squares of the differences between the observed responses in the given dataset and those predicted by the linear model. (i.e.  $RSS = \sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p(\beta_{j}x_{ij}))^2$, where $\beta_0 ... \beta_p$ are the coefficients estimate)

**_Ordinary Least Squares (OLS)_**
This is a common liear regression model, and we use it as a benchmark to evaluate the performance of the other 4 regression models. 

### Shrinkage methods

Shrinkage methods invovles fitting a model with all P predictor. However, the estimated coefficients are shrunken towards 0 relative to the least squares estimates. Shrinkage has the effect of reducing variance. Since after the process of shrinkage, some of the coefficients might be exactly 0, it also performs variable selection.

**_Ridge Regression (Ridge)_**

Ridge regression is very similar to least square, except that the coefficients are estimatated by minimizing $RSS + \lambda \sum \beta_j^2$. This equation involves the minimization of two criterias, the first one is the same as OLS, ridge regersssion seeks to fit the data well by minimizing RSS. The second part of this equation, $\lambda \sum \beta_j^2$, is called the _shrinkage penalty_. And the shrinkage penalty gets smaller as coefficients get closer to 0. $\lambda$ here is called the tuning parameter, and $\lambda \geq 0$. When $\lambda = 0$, Ridge regression is the same as OLS. As $\lambda$ grows larger, the impact of shrinkage penalty grows bigger as well. The use of tuning parameter $\lambda$ is also where Ridge regression outperforms OLS, and the mechnaism's name is called _bias variance tradeoff_. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias

**_Lasso Regression (Lasso)_**

Lasso regression is an alternative to Ridge and it overcomes oen of Ridge's disadvantage, that even though it uses all P predictors and even though it moves all coefficients close to 0, it won't set any of them to be exactly 0 unless $\lambda$ is infinity and this disadvantage creates obstacles on model interpretation when P gets really large. Lasso regression minimizes $RSS + \lambda \sum |{\beta_j}|$. And it is the 2nd term that forces some of the coefficients estimates to be exactly 0.

### Dimension Reduction methods

Dimension reduction works by _projecting_ the P predictors onto a M-dimensional space, where M < P. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.

**_Principal Components Regression (PCR)_**

The _first principal component_ direction of the data is that along which the observation vary the _most_. The PCR approach involves constructing the first M principal components, $Z_1, Z_2,...,Z_M$, and then using those principle components as predictors in a linear regression model that uses least squares method. This approach reduces dimension because of the fact that usually a samller amount ($M < P$) of principal components are sufficient to explain most of the variabilities in the dataset, as well as the relationship with the response. PCR is an _unsupervised_ approach because in the process of finding the first M principal components that best describes the P predictors, the response Y is not used to help determine the principal component direction.

**_Partial Least Square Regression (PLSR)_**

PLSR is a _supervised_ alternative to PCR. It first identifies a new set of features $Z_1, Z_2,...,Z_M$ that are linear combinations of the original features, and then fits a linear model using least squares method with those M features. The difference is PLSR identifies $Z_1, Z_2,...Z_M$ in a _supervised_ way, which means it makes use of the response Y while identifying new features, therefore the new features not only approximate the old feature well enough, they are also related the response. 




#Analysis 

### Data Processing

In order to conduct our analysis, we first began with some pre-modeling data processing. The two main actions we performed before applying our models were: 

1. Dummying out categorical variable 
2. Mean centering and standardizing all the variables 

For step 1, we took our categorical, or qualitative variables, namely `Student`, `Married`, `Gender`, and `Ethnicity` and *dummied* them out. What this means is that the values for the variables were factored and then given binary indicators. The reason for this is you can not apply regression models, specifically the lasso and ridge regressions that we used from the `glmnet` package, to variables that are non numeric. We used the `model.matrix()` function to perform step 1. 

For step 2, we standardized our data so that each variable would have a mean of 0 and standard deviation of 1. The purpose of this is to standardize across different scales, so that coefficients such as ${\hat\beta_0}$ do not vary based on whether they're calculated in pounds or ounces, etc. To do so, we used the `scale()` function.

### Training and Testing Data Sets 

Another step we took before building our model was to take a set of data for model building and another for testing model performance. The set of data used to build the model, our *training set*, was 300 events randomly selected using the `sample()` function. For our test set, we randomly selected 100 events, this time using the `set.seed()` function for reproducibility. 

### Model Building Process 

# Results
# Conclusions
